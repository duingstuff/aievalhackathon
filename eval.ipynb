{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GsE2oHdaOQg0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your browser has been opened to visit:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=PQQrVSNVxh0pI6M4Botw6XtdMgRhrP&access_type=offline&code_challenge=mkaFsQA8NMnJ8JWIBhXIpvqHOfi10QVmdqHRxG0QspU&code_challenge_method=S256\n",
            "\n",
            "\n",
            "You are now logged in as [duyguider@google.com].\n",
            "Your current project is [claims-assistant-fsa].  You can change this setting by running:\n",
            "  $ gcloud config set project PROJECT_ID\n",
            "\n",
            "\n",
            "Updates are available for some Google Cloud CLI components.  To install them,\n",
            "please run:\n",
            "  $ gcloud components update\n",
            "\n",
            "\n",
            "\n",
            "To take a quick anonymous survey, run:\n",
            "  $ gcloud survey\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!gcloud auth login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "UmoJCyc_OGmT",
        "outputId": "8a266eaa-0bf0-4517-a964-1963ca3b95be"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import requests\n",
        "import json\n",
        "from agents import Runner, Agent\n",
        "from datetime import datetime\n",
        "from agents.sessions.in_memory_session_service import InMemorySessionService\n",
        "from agents.artifacts.in_memory_artifact_service import InMemoryArtifactService\n",
        "from google.generativeai import GenerativeModel, Part, Content, GenerationConfig  # Vertex AI Gemini\n",
        "import google.cloud.aiplatform as aiplatform\n",
        "\n",
        "\n",
        "# --- Utility Functions ---\n",
        "\n",
        "def extract_responses_from_code(agent_code: str) -> list:\n",
        "    \"\"\"\n",
        "    Extracts sample responses from the agent's code (Simplified Placeholder).\n",
        "    This is VERY difficult to do reliably without running the code.\n",
        "    This version uses a very simple regex, which will likely fail in many cases.\n",
        "    \"\"\"\n",
        "    # VERY rudimentary attempt to find 'text' parts in responses.\n",
        "    matches = re.findall(r\"text=['\\\"](.*?)['\\\"]\", agent_code, re.DOTALL)\n",
        "    return matches\n",
        "\n",
        "\n",
        "def parse_evaluation(evaluation_text: str) -> dict:\n",
        "    \"\"\"Parses the evaluation text from the Gemini model (Placeholder).\"\"\"\n",
        "    scores = {}\n",
        "    try:\n",
        "        # Attempt to parse as JSON (ideal case)\n",
        "        scores = json.loads(evaluation_text)\n",
        "    except json.JSONDecodeError:\n",
        "        # Fallback: Use regex to find scores (less reliable)\n",
        "        match = re.search(r\"fluency:\\s*(\\d+).*coherence:\\s*(\\d+).*relevance:\\s*(\\d+).*helpfulness:\\s*(\\d+)\", evaluation_text, re.IGNORECASE)\n",
        "        if match:\n",
        "            scores = {\n",
        "                \"fluency\": int(match.group(1)),\n",
        "                \"coherence\": int(match.group(2)),\n",
        "                \"relevance\": int(match.group(3)),\n",
        "                \"helpfulness\": int(match.group(4)),\n",
        "            }\n",
        "    return scores\n",
        "\n",
        "\n",
        "def google_search(query: str) -> dict:\n",
        "    \"\"\"Performs a Google Search (using the Custom Search JSON API).\"\"\"\n",
        "    try:\n",
        "        url = f\"https://www.googleapis.com/customsearch/v1?key={os.environ['GOOGLE_SEARCH_API_KEY']}&cx={os.environ['SEARCH_ENGINE_ID']}&q={query}\"\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "        return response.json()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error during Google Search: {e}\")  # Log the error\n",
        "        return {} # Return an empty dict\n",
        "    except KeyError as e:\n",
        "        print(f\"Missing environment variable: {e}\")\n",
        "        return {}\n",
        "\n",
        "\n",
        "def extract_claims(text: str) -> list:\n",
        "    \"\"\"Extracts factual claims from the agent's response (Placeholder).\"\"\"\n",
        "    # Very basic sentence splitting.  Real-world claim extraction is MUCH harder.\n",
        "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
        "    return sentences\n",
        "\n",
        "\n",
        "def verify_claim(claim: str, search_results: dict) -> bool:\n",
        "    \"\"\"Verifies a claim against search results (Placeholder).\"\"\"\n",
        "    # Extremely simplified verification.  Real-world verification is complex.\n",
        "    if not search_results or 'items' not in search_results:\n",
        "        return False #Unable to verify\n",
        "\n",
        "    for item in search_results['items']:\n",
        "        if claim.lower() in item.get('title', '').lower() or claim.lower() in item.get('snippet', '').lower():\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "# --- Tool Definitions ---\n",
        "\n",
        "def generation_evaluator(agent_code: str, sample_interactions: list = None) -> dict:\n",
        "    \"\"\"Evaluates the generation quality of the agent's responses.\"\"\"\n",
        "    model = GenerativeModel(\"gemini-1.5-pro-002\")\n",
        "\n",
        "    if sample_interactions is None:\n",
        "        sample_responses = extract_responses_from_code(agent_code)\n",
        "    else:\n",
        "        sample_responses = [interaction['output'] for interaction in sample_interactions]\n",
        "\n",
        "    scores = {}\n",
        "    for i, response in enumerate(sample_responses):\n",
        "        prompt = f\"\"\"Evaluate the following response based on fluency, coherence, relevance, and helpfulness.  Provide a score from 0 to 10 for each, where 10 is best.\n",
        "\n",
        "        Response:\n",
        "        {response}\n",
        "\n",
        "        Scores (in JSON format):\n",
        "        \"\"\"\n",
        "        try:\n",
        "            evaluation = model.generate_content(prompt)\n",
        "            scores[f\"response_{i}\"] = parse_evaluation(evaluation.text)\n",
        "        except Exception as e:\n",
        "             print(f\"Error during generation evaluation: {e}\") #Error handeling\n",
        "             scores[f\"response_{i}\"] =  {\"fluency\": 0, \"coherence\": 0, \"relevance\": 0, \"helpfulness\": 0}\n",
        "    return scores\n",
        "\n",
        "\n",
        "def context_evaluator(agent_code: str, sample_interactions: list) -> float:\n",
        "    \"\"\"Evaluates the agent's ability to maintain and use context.\"\"\"\n",
        "    # Placeholder:  In a real implementation, this would involve running the agent's code.\n",
        "    # For now, we'll just return a placeholder score based on whether interactions are provided.\n",
        "\n",
        "    if not sample_interactions:\n",
        "         return 0.0\n",
        "\n",
        "    model = GenerativeModel(\"gemini-1.5-pro-002\")\n",
        "\n",
        "    conversation_history = \"\"\n",
        "    total_context_score = 0\n",
        "\n",
        "    for interaction in sample_interactions:\n",
        "        user_input = interaction['input']\n",
        "        agent_response = interaction['output']\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        You are evaluating an AI agent's ability to maintain context in a conversation.\n",
        "        Here is the conversation history so far:\n",
        "        {conversation_history}\n",
        "\n",
        "        The user's latest input is:\n",
        "        {user_input}\n",
        "\n",
        "        The agent's response is:\n",
        "        {agent_response}\n",
        "\n",
        "        Rate the agent's response on a scale of 0 to 10 (10 being best) based on how well it uses and maintains context from the previous conversation. Return a single number.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "          evaluation = model.generate_content(prompt)\n",
        "          score = int(evaluation.text.strip())  # Try to convert to integer\n",
        "          total_context_score += score\n",
        "        except Exception as e:\n",
        "            print(f\"Error during evaluation: {e}\")\n",
        "            total_context_score += 0\n",
        "\n",
        "        # Update conversation history\n",
        "        conversation_history += f\"User: {user_input}\\nAgent: {agent_response}\\n\"\n",
        "\n",
        "\n",
        "    return total_context_score / len(sample_interactions) if sample_interactions else 0.0\n",
        "\n",
        "\n",
        "\n",
        "def groundness_evaluator(agent_code: str, sample_interactions: list) -> float:\n",
        "    \"\"\"Evaluates the factual accuracy of the agent's responses.\"\"\"\n",
        "    if not sample_interactions:\n",
        "        return 0.0\n",
        "\n",
        "    scores = []\n",
        "    for interaction in sample_interactions:\n",
        "        agent_response = interaction['output']\n",
        "        claims = extract_claims(agent_response)\n",
        "\n",
        "        for claim in claims:\n",
        "            try:\n",
        "                search_results = google_search(claim)\n",
        "                if verify_claim(claim, search_results):\n",
        "                    scores.append(10)\n",
        "                else:\n",
        "                    scores.append(0)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during groundness evaluation: {e}\")\n",
        "                scores.append(0)  # Assume ungrounded if error\n",
        "\n",
        "    return sum(scores) / len(scores) if scores else 0.0\n",
        "\n",
        "\n",
        "def function_caller_evaluator(agent_code: str, sample_interactions: list) -> float:\n",
        "    \"\"\"Evaluates the agent's ability to select and use the correct functions.\"\"\"\n",
        "    # Placeholder: Requires dynamic execution of the agent code.\n",
        "    if not sample_interactions:\n",
        "      return 0.0\n",
        "\n",
        "    model = GenerativeModel(\"gemini-1.5-pro-002\")\n",
        "    total_function_score = 0\n",
        "\n",
        "    for interaction in sample_interactions:\n",
        "        user_input = interaction['input']\n",
        "        agent_response = interaction['output']\n",
        "\n",
        "        prompt = f\"\"\"You are evaluating an AI agent's ability to use function calls.\n",
        "        User Input: {user_input}\n",
        "        Agent response: {agent_response}\n",
        "        Based on the user input, analyze agent's response and rate from 0 to 10 if used the correct functions to get the response (where 10 is best). Return a single number.\n",
        "        \"\"\"\n",
        "        try:\n",
        "          evaluation = model.generate_content(prompt)\n",
        "          score = int(evaluation.text.strip())  # Try to convert to integer\n",
        "          total_function_score += score\n",
        "        except Exception as e:\n",
        "            print(f\"Error during evaluation: {e}\")\n",
        "            total_function_score += 0\n",
        "    return total_function_score / len(sample_interactions) if sample_interactions else 0.0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- Agent Definition ---\n",
        "judge_agent = Agent(\n",
        "    model=\"gemini-1.5-pro-002\",  # Use a strong model\n",
        "    name=\"AgentEvaluator\",\n",
        "    instruction=\"\"\"\n",
        "        You are an agent designed to evaluate the quality and correctness of other AI agents.\n",
        "        You will receive the source code of a target agent as input, and optionally, sample interactions.\n",
        "        Analyze the code and use the provided tools to assess its:\n",
        "        - Generation quality (fluency, coherence, relevance, helpfulness)\n",
        "        - Context handling (ability to maintain conversation state)\n",
        "        - Groundness (factual accuracy)\n",
        "        - Function selection (correct use of tools)\n",
        "\n",
        "        Report the individual scores from each tool and the final score. The final score should be the average of generation, context, groundness and function caller scores.  Each of those scores should be a single number between 0 and 10.\n",
        "    \"\"\",\n",
        "    tools=[\n",
        "        generation_evaluator,\n",
        "        context_evaluator,\n",
        "        groundness_evaluator,\n",
        "        function_caller_evaluator,\n",
        "        google_search  # Added to be used by groundness_evaluator\n",
        "    ],\n",
        "      flow='sequential',\n",
        ")\n",
        "\n",
        "# --- Agent Initialization ---\n",
        "session_service = InMemorySessionService()\n",
        "artifact_service = InMemoryArtifactService()\n",
        "runner = Runner(app_name=\"AgentEvaluator\", agent=judge_agent, artifact_service=artifact_service, session_service=session_service)\n",
        "session = session_service.create(app_name=\"AgentEvaluator\", user_id=\"1\")\n",
        "\n",
        "\n",
        "def run_prompt(agent_code: str, sample_interactions: list = None):\n",
        "     # Use Vertex AI Content objects\n",
        "    content = Content(role='user', parts=[Part.from_text(agent_code)])\n",
        "    #print(content) # Remove to avoid noisy output.\n",
        "    final_response = None #To get the last response\n",
        "    for event in runner.run(\n",
        "      session=session,\n",
        "      new_message=content,\n",
        "      tool_args={\"sample_interactions\": sample_interactions}\n",
        "    ):\n",
        "        if event.content:\n",
        "            #print(event.content)  # Optional: Print intermediate steps (Vertex AI format)\n",
        "            final_response = event.content\n",
        "\n",
        "    return final_response\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Setup API Keys and Vertex AI ---\n",
        "    # Best practice: Load from environment variables\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_GOOGLE_API_KEY\"  # For Gemini API (still needed for google_search)\n",
        "    os.environ[\"GOOGLE_SEARCH_API_KEY\"] = \"YOUR_GOOGLE_SEARCH_API_KEY\"  # For Google Custom Search API\n",
        "    os.environ[\"SEARCH_ENGINE_ID\"] = \"YOUR_SEARCH_ENGINE_ID\"  # Your Custom Search Engine ID\n",
        "    os.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"YOUR_GOOGLE_CLOUD_PROJECT\" #Your project id\n",
        "    os.environ[\"GOOGLE_CLOUD_LOCATION\"] = \"us-central1\" #The region\n",
        "\n",
        "    # Initialize Vertex AI\n",
        "    aiplatform.init(project=os.environ[\"GOOGLE_CLOUD_PROJECT\"], location=os.environ[\"GOOGLE_CLOUD_LOCATION\"])\n",
        "\n",
        "\n",
        "    # --- Load Target Agent's Code and sample interactions ---\n",
        "    # Example usage (replace with the path to your agent's code):\n",
        "    try:\n",
        "      with open(\"target_agent.py\", \"r\") as f:\n",
        "          target_agent_code = f.read()\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: target_agent.py not found. Please provide the correct path.\")\n",
        "        exit(1) #Exit if can't continue\n",
        "\n",
        "    sample_interactions = [\n",
        "        {'input': \"What's the weather like today?\",\n",
        "         'output': \"I'm sorry, I don't have the ability to look up weather information.\"},\n",
        "        {'input': \"What's 2 + 2?\", 'output': \"2 + 2 = 4\"},\n",
        "        {'input': \"Can you tell me the capital of France?\", 'output': \"The capital of France is Paris.\"},\n",
        "        {'input': \"And what is the population of Paris?\", 'output': \"The population of Paris is about 2.1 million people.\"}, #Context question\n",
        "        {'input':\"What is the exchange rate from USD to EUR?\", 'output': \"The exchange rate of USD to EUR is bla bla.\"}, # Test the function caller\n",
        "\n",
        "    ]\n",
        "\n",
        "    # --- Run the Evaluation ---\n",
        "    evaluation_result = run_prompt(target_agent_code, sample_interactions)\n",
        "\n",
        "    # --- Process and Print Results ---\n",
        "    if evaluation_result and evaluation_result.parts:\n",
        "        # Extract the final text response\n",
        "        final_text = evaluation_result.parts[0].text\n",
        "        print(\"Evaluation Results:\")\n",
        "        print(final_text)\n",
        "\n",
        "\n",
        "        #Attempt to extract scores.\n",
        "        try:\n",
        "\n",
        "          match = re.search(r\"Generation quality score:\\s*([\\d\\.]+).*Context handling score:\\s*([\\d\\.]+).*Groundness score:\\s*([\\d\\.]+).*Function selection score:\\s*([\\d\\.]+).*Final score:\\s*([\\d\\.]+)\", final_text, re.IGNORECASE)\n",
        "          if match:\n",
        "            print(\"\\nExtracted Scores:\")\n",
        "            print(f\"  Generation Quality: {float(match.group(1))}\")\n",
        "            print(f\"  Context Handling: {float(match.group(2))}\")\n",
        "            print(f\"  Groundness: {float(match.group(3))}\")\n",
        "            print(f\"  Function Selection: {float(match.group(4))}\")\n",
        "            print(f\"  Final Score: {float(match.group(5))}\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\nCould not extract individual scores: {e}\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(\"No evaluation results returned.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zq_I6jBfOI00"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "3.12.0",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
